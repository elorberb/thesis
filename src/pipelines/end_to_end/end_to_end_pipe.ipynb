{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplanning:\\n- load the detectron faster rcnn model and perform detections on a single image\\n- chosen model faster rcnn R50 C4 x1\\n- after i have the detection i should use them for the classification\\n- after i have images to use for the classification i should use the alexnet model to classify them.\\n- after that save the results\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "planning:\n",
    "- load the detectron faster rcnn model and perform detections on a single image\n",
    "- chosen model faster rcnn R50 C4 x1\n",
    "- after i have the detection i should use them for the classification\n",
    "- after i have images to use for the classification i should use the alexnet model to classify them.\n",
    "- after that save the results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Apr_17_19:19:55_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.40\n",
      "Build cuda_12.5.r12.5/compiler.34177558_0\n",
      "torch:  2.3 ; cuda:  12.1\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "# load the detectron model\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "\n",
    "# checking the version and if we have cuda available\n",
    "from src.segmentation.framework_handlers.detectron2_handler import print_version_info\n",
    "\n",
    "!nvcc --version\n",
    "print_version_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# import required functions, classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "\n",
    "detection_model_config = {\n",
    "    \"model_name\": \"faster_rcnn_R_50_C4_1x\",\n",
    "    \"checkpoint\": \"/home/etaylor/code_projects/thesis/checkpoints/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/29-04-2024_16-09-41/model_final.pth\",\n",
    "    \"yaml_file\": \"/home/etaylor/code_projects/thesis/checkpoints/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/29-04-2024_16-09-41/config.yaml\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/16/2024 17:39:23 - WARNING - fvcore.common.config -   Loading config /home/etaylor/code_projects/thesis/checkpoints/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/29-04-2024_16-09-41/config.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/etaylor/code_projects/thesis/checkpoints/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/29-04-2024_16-09-41/config.yaml not available in Model Zoo!\n",
      "\u001b[32m[07/16 17:39:24 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/etaylor/code_projects/thesis/checkpoints/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/29-04-2024_16-09-41/model_final.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/16/2024 17:39:24 - INFO - fvcore.common.checkpoint -   [Checkpointer] Loading from /home/etaylor/code_projects/thesis/checkpoints/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/29-04-2024_16-09-41/model_final.pth ...\n",
      "07/16/2024 17:39:24 - WARNING - sahi.models.detectron2 -   Attribute 'thing_classes' does not exist in the metadata of dataset 'etaylor/cannabis_patches_train_26-04-2024_15-44-44': metadata is empty.\n"
     ]
    }
   ],
   "source": [
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='detectron2',\n",
    "    model_path=detection_model_config['checkpoint'],\n",
    "    config_path=detection_model_config['yaml_file'],\n",
    "    confidence_threshold=0.5,\n",
    "    image_size=512,\n",
    "    device=\"cuda:0\", # or 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "colors of export_visuals function:\n",
    "- blue - clear trichomes\n",
    "- orange - cloudy trichomes\n",
    "- purple - amber trichomes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 48 slices.\n",
      "Time taken to process image: 14.385375738143921\n"
     ]
    }
   ],
   "source": [
    "raw_image_path = \"/home/etaylor/images/assessing_cannabis_experiment_images/day_5_2024_06_13/greenhouse/138/IMG_6652.JPG\"\n",
    "\n",
    "start = time.time()\n",
    "result = get_sliced_prediction(\n",
    "raw_image_path,\n",
    "detection_model,\n",
    "slice_height = 512,\n",
    "slice_width = 512,\n",
    "overlap_height_ratio = 0,\n",
    "overlap_width_ratio = 0,\n",
    "verbose=True,\n",
    ")\n",
    "end_time = time.time() - start\n",
    "print(f\"Time taken to process image: {end_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with AlexNet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping from classification model to object detection model\n",
    "classification_to_detection_mapping = {\n",
    "    0: 3,  # Amber (classification) -> Amber (object detection)\n",
    "    1: 1,  # Clear (classification) -> Clear (object detection)\n",
    "    2: 2   # Cloudy (classification) -> Cloudy (object detection)\n",
    "}\n",
    "\n",
    "classification_dataset_config = {\n",
    "    'train': '/home/etaylor/code_projects/thesis/segments/etaylor_cannabis_patches_train_26-04-2024_15-44-44/trichome_dataset_01',\n",
    "    'test': '/home/etaylor/code_projects/thesis/segments/etaylor_cannabis_patches_test_26-04-2024_15-44-44/ground_truth_trichomes_datasets/trichome_dataset_01',\n",
    "}\n",
    "\n",
    "classification_models_path = \"/home/etaylor/code_projects/thesis/checkpoints/image_classification_models\"\n",
    "\n",
    "classification_model_config = {\n",
    "    'model_name': 'alexnet',\n",
    "    'model': models.alexnet,\n",
    "    'checkpoint': f'{classification_models_path}/alexnet_model_12_7_24.pkl'\n",
    "}\n",
    "\n",
    "# transformation and image space conversion\n",
    "def custom_transform(size):\n",
    "    return Resize(size, method='pad', pad_mode='zeros')\n",
    "\n",
    "class RGB2HSV(Transform):\n",
    "    def encodes(self, img: PILImage): \n",
    "        return rgb2hsv(img)\n",
    "    \n",
    "    \n",
    "global_item_tfms=custom_transform(size=128),  # Resize and HSV transform\n",
    "global_batch_tfms=[\n",
    "    RGB2HSV(),\n",
    "    *aug_transforms(size=128, flip_vert=True, max_rotate=10),\n",
    "    Brightness(max_lighting=0.2, p=0.75),\n",
    "    Contrast(max_lighting=0.2, p=0.75),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load models functions\n",
    "def load_detection_model(model_config, patch_size=512):\n",
    "    logger.info(\"Loading the model.\")\n",
    "    detection_model = AutoDetectionModel.from_pretrained(\n",
    "        model_type='detectron2',\n",
    "        model_path=model_config['checkpoint'],\n",
    "        config_path=model_config['yaml_file'],\n",
    "        confidence_threshold=0.5,\n",
    "        image_size=patch_size,\n",
    "        device=\"cuda:0\",  # or 'cpu'\n",
    "    )\n",
    "    return detection_model\n",
    "\n",
    "# Load classification model\n",
    "def load_classification_model(classification_model_config):\n",
    "    print(\"Loading the classification model.\")\n",
    "    learn = load_learner(classification_model_config['checkpoint'])\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# process image functions\n",
    "def perform_object_detection(image_path, detection_model, patch_size=512):\n",
    "    logger.info(f\"Performing object detection on image: {os.path.basename(image_path)}\")\n",
    "    start_time = time.time()\n",
    "    result = get_sliced_prediction(\n",
    "        image_path,\n",
    "        detection_model,\n",
    "        slice_height=patch_size,\n",
    "        slice_width=patch_size,\n",
    "        overlap_height_ratio=0,\n",
    "        overlap_width_ratio=0,\n",
    "        verbose=True,\n",
    "    )\n",
    "    detection_time = time.time() - start_time\n",
    "    logger.info(f\"Time taken for object detection on image {os.path.basename(image_path)}: {detection_time:.2f} seconds\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_large_objects(predictions, size_threshold_ratio=10):\n",
    "    sizes = [(pred.bbox.maxx - pred.bbox.minx) * (pred.bbox.maxy - pred.bbox.miny) for pred in predictions]\n",
    "    if sizes:\n",
    "        median_size = np.median(sizes)\n",
    "        filtered_predictions = [pred for pred in predictions if (pred.bbox.maxx - pred.bbox.minx) * (pred.bbox.maxy - pred.bbox.miny) <= median_size * size_threshold_ratio]\n",
    "        return filtered_predictions\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def export_visuals(result, output_dir, base_file_name, stage):\n",
    "    stage_output_dir = os.path.join(output_dir, f\"{base_file_name}_{stage}\")\n",
    "    os.makedirs(stage_output_dir, exist_ok=True)\n",
    "    result.export_visuals(\n",
    "        export_dir=stage_output_dir,\n",
    "        text_size=1,\n",
    "        rect_th=2,\n",
    "        hide_labels=True,\n",
    "        hide_conf=True,\n",
    "        file_name=base_file_name\n",
    "    )\n",
    "    logger.info(f\"Exported {stage} visuals for image {base_file_name}\")\n",
    "    \n",
    "    \n",
    "def extend_bounding_box(x_min, y_min, x_max, y_max, image_width, image_height, margin=0.1):\n",
    "    bbox_width = x_max - x_min\n",
    "    bbox_height = y_max - y_min\n",
    "    \n",
    "    x_min_extended = max(0, x_min - int(margin * bbox_width))\n",
    "    y_min_extended = max(0, y_min - int(margin * bbox_height))\n",
    "    x_max_extended = min(image_width, x_max + int(margin * bbox_width))\n",
    "    y_max_extended = min(image_height, y_max + int(margin * bbox_height))\n",
    "    \n",
    "    return x_min_extended, y_min_extended, x_max_extended, y_max_extended\n",
    "\n",
    "\n",
    "def crop_image(image, x_min, y_min, x_max, y_max):\n",
    "    return image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "\n",
    "def apply_transformations(cropped_image):\n",
    "    cropped_pil_image = PILImage.create(cropped_image)\n",
    "    \n",
    "    for item_tfms in global_item_tfms:\n",
    "        cropped_pil_image = item_tfms(cropped_pil_image)\n",
    "    \n",
    "    return Image.fromarray(np.array(cropped_pil_image))\n",
    "\n",
    "\n",
    "def classify_cropped_image(cropped_fastai_image, classification_model):\n",
    "    _, classification_model_pred_class_id, _ = classification_model.predict(cropped_fastai_image)\n",
    "    return int(classification_model_pred_class_id)\n",
    "\n",
    "\n",
    "def plot_classified_object(cropped_image, detection_class_name, classification_class_name):\n",
    "    plt.title(f\"Detected Class (Faster R-CNN): {detection_class_name}\\nPredicted Class (AlexNet): {classification_class_name}\")\n",
    "    plt.imshow(cropped_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def classify_objects(image_path, result, classification_model):\n",
    "    logger.info(\"Classifying detected objects.\")\n",
    "    image = cv2.imread(image_path)\n",
    "    image_height, image_width, _ = image.shape\n",
    "    start_classification = time.time()\n",
    "\n",
    "    for prediction in result.object_prediction_list:\n",
    "        \n",
    "        # Save the original class id for the detection model\n",
    "        detection_model_pred_class_id = prediction.category.id\n",
    "        print(f\"pred id before: {detection_model_pred_class_id}\")\n",
    "        \n",
    "        # Get the original bounding box coordinates\n",
    "        x_min = int(prediction.bbox.minx)\n",
    "        y_min = int(prediction.bbox.miny)\n",
    "        x_max = int(prediction.bbox.maxx)\n",
    "        y_max = int(prediction.bbox.maxy)\n",
    "        \n",
    "        # Extend bounding box by 10%\n",
    "        x_min_extended, y_min_extended, x_max_extended, y_max_extended = extend_bounding_box(\n",
    "            x_min, y_min, x_max, y_max, image_width, image_height\n",
    "        )\n",
    "        \n",
    "        # Crop the extended bounding box from the original image\n",
    "        cropped_image = crop_image(image, x_min_extended, y_min_extended, x_max_extended, y_max_extended)\n",
    "        \n",
    "        # Apply transformations\n",
    "        cropped_fastai_image = apply_transformations(cropped_image)\n",
    "        \n",
    "        # Classify the cropped image\n",
    "        classification_model_pred_class_id = classify_cropped_image(cropped_fastai_image, classification_model)\n",
    "                \n",
    "        # Get the corresponding classification model class id for the detection model (visuals purposes)\n",
    "        prediction.category.id = classification_to_detection_mapping[classification_model_pred_class_id]\n",
    "        print(f\"pred id after {prediction.category.id}\")\n",
    "        \n",
    "        labels = [\"Clear\", \"Cloudy\", \"Amber\"]\n",
    "        faster_rcnn_class_name = labels[detection_model_pred_class_id - 1]\n",
    "        alexnet_class_name = labels[prediction.category.id - 1]\n",
    "        \n",
    "        # Plot the classified object\n",
    "        plot_classified_object(cropped_image, faster_rcnn_class_name, alexnet_class_name)\n",
    "        \n",
    "        logger.info(f\"Detected Class (Faster R-CNN): {detection_model_pred_class_id} {faster_rcnn_class_name}\")\n",
    "        logger.info(f\"Predicted Class (AlexNet): {prediction.category.id} {alexnet_class_name}\")\n",
    "\n",
    "    end_classification = time.time() - start_classification\n",
    "    logger.info(f\"Time taken for classification: {end_classification:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_and_classify_image(image_path, detection_model, classification_model, patch_size, output_dir, base_file_name):\n",
    "    result = perform_object_detection(image_path, detection_model, patch_size)\n",
    "    export_visuals(result, output_dir, base_file_name, \"pre_classification\")\n",
    "    \n",
    "    filtered_predictions = filter_large_objects(result.object_prediction_list)\n",
    "    result.object_prediction_list = filtered_predictions\n",
    "    \n",
    "    classify_objects(image_path, result, classification_model)\n",
    "    export_visuals(result, output_dir, base_file_name, \"post_classification\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/16/2024 19:01:29 - INFO - __main__ -   Loading the model.\n",
      "07/16/2024 19:01:29 - WARNING - fvcore.common.config -   Loading config /home/etaylor/code_projects/thesis/checkpoints/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/29-04-2024_16-09-41/config.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/etaylor/code_projects/thesis/checkpoints/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/29-04-2024_16-09-41/config.yaml not available in Model Zoo!\n",
      "\u001b[32m[07/16 19:01:29 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/etaylor/code_projects/thesis/checkpoints/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/29-04-2024_16-09-41/model_final.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/16/2024 19:01:29 - INFO - fvcore.common.checkpoint -   [Checkpointer] Loading from /home/etaylor/code_projects/thesis/checkpoints/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/29-04-2024_16-09-41/model_final.pth ...\n",
      "07/16/2024 19:01:30 - WARNING - sahi.models.detectron2 -   Attribute 'thing_classes' does not exist in the metadata of dataset 'etaylor/cannabis_patches_train_26-04-2024_15-44-44': metadata is empty.\n",
      "07/16/2024 19:01:30 - INFO - __main__ -   Performing object detection on image: IMG_6652.JPG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the classification model.\n",
      "Performing prediction on 48 slices.\n"
     ]
    }
   ],
   "source": [
    "raw_image_path = \"/home/etaylor/images/assessing_cannabis_experiment_images/day_5_2024_06_13/greenhouse/138/IMG_6652.JPG\"\n",
    "\n",
    "res = process_and_classify_image(\n",
    "    image_path=raw_image_path,\n",
    "    detection_model=load_detection_model(detection_model_config),\n",
    "    classification_model=load_classification_model(classification_model_config),\n",
    "    output_dir=\"/home/etaylor/code_projects/thesis/src/pipelines/end_to_end/testing_end_to_end_pipe\",\n",
    "    base_file_name=\"IMG_6652\",\n",
    "    patch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectPrediction<\n",
       "    bbox: BoundingBox: <(2241.2849731445312, 1967.7375793457031, 2278.209197998047, 2008.1548156738281), w: 36.924224853515625, h: 40.417236328125>,\n",
       "    mask: None,\n",
       "    score: PredictionScore: <value: 0.964286744594574>,\n",
       "    category: Category: <id: 1, name: 1>>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.object_prediction_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification dataset transformations\n",
    "# Define the custom transformations\n",
    "def custom_transform(size):\n",
    "    return Resize(size, method='pad', pad_mode='zeros')\n",
    "\n",
    "class RGB2HSV(Transform):\n",
    "    def encodes(self, img: PILImage):\n",
    "        return rgb2hsv(np.array(img))\n",
    "\n",
    "# Apply global transformations for inference\n",
    "global_item_tfms = [custom_transform(size=128)]\n",
    "global_batch_tfms = [\n",
    "    RGB2HSV(),\n",
    "    *aug_transforms(size=128, flip_vert=True, max_rotate=10),\n",
    "    Brightness(max_lighting=0.2, p=0.75),\n",
    "    Contrast(max_lighting=0.2, p=0.75),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing object detection...\")\n",
    "start = time.time()\n",
    "result = get_sliced_prediction(\n",
    "    raw_image_path,\n",
    "    detection_model,\n",
    "    slice_height=512,\n",
    "    slice_width=512,\n",
    "    overlap_height_ratio=0,\n",
    "    overlap_width_ratio=0,\n",
    "    verbose=True,\n",
    ")\n",
    "end_time = time.time() - start\n",
    "print(f\"Time taken to for object detection model: {end_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original image\n",
    "image = cv2.imread(raw_image_path)\n",
    "\n",
    "# initialize a time for the classification\n",
    "start_classification = time.time()\n",
    "# Iterate over detected objects and classify them\n",
    "for i, prediction in enumerate(result.object_prediction_list):\n",
    "    # print(f\"Processing object {i+1}/{len(result.object_prediction_list)}...\")\n",
    "    # Extract bounding box coordinates\n",
    "    x_min = int(prediction.bbox.minx)\n",
    "    y_min = int(prediction.bbox.miny)\n",
    "    x_max = int(prediction.bbox.maxx)\n",
    "    y_max = int(prediction.bbox.maxy)\n",
    "\n",
    "    # Crop the detected object from the original image\n",
    "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    # Plot the cropped bounding box image\n",
    "    # plt.figure(figsize=(5, 5))\n",
    "    # plt.imshow(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
    "    # plt.axis('off')\n",
    "    # plt.title(\"Cropped Bounding Box Image\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Convert the cropped image to PIL format for Fastai\n",
    "    cropped_pil_image = PILImage.create(cropped_image)\n",
    "\n",
    "    # Apply the same transformations as during training\n",
    "    for item_tfms in global_item_tfms:\n",
    "        cropped_pil_image = item_tfms(cropped_pil_image)\n",
    "\n",
    "    # Convert the transformed image to a Fastai Image\n",
    "    cropped_fastai_image = Image.fromarray(np.array(cropped_pil_image))\n",
    "\n",
    "    # Perform classification on the cropped image\n",
    "    pred_class, classification_model_pred_class_id, outputs = classification_model.predict(cropped_fastai_image)\n",
    "    classification_model_pred_class_id = int(classification_model_pred_class_id)  # Convert tensor to int\n",
    "\n",
    "    # Map the classification result to the detection model class\n",
    "    detection_class = classification_to_detection_mapping[classification_model_pred_class_id]\n",
    "    faster_rcnn_class_name = [\"Clear\", \"Cloudy\", \"Amber\"][prediction.category.id - 1]\n",
    "    alexnet_class_name = [\"Amber\", \"Clear\", \"Cloudy\"][classification_model_pred_class_id]\n",
    "    \n",
    "    print(f\"Detected Class (Faster R-CNN): {faster_rcnn_class_name}\")\n",
    "    print(f\"Predicted Class (AlexNet): {alexnet_class_name}\")\n",
    "    \n",
    "end = time.time() - start_classification\n",
    "print(f\"Time taken for classification: {end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron_fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
