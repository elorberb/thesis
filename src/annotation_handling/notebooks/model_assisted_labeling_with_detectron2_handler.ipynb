{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wnygc-uoVLfx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/etaylor/.conda/envs/detectron/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from src.annotation_handling.segmentsai_handler import SegmentsAIHandler\n",
        "from src.pipelines import model_assist_label_pipeline as pipe \n",
        "from src.segmentation.framework_handlers.detectron2_handler import Detectron2Handler\n",
        "import config\n",
        "\n",
        "from segments import SegmentsDataset\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw2JEh1Sk52I"
      },
      "source": [
        "#### Setup Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zgjm9kb2M9lL"
      },
      "outputs": [],
      "source": [
        "segmentsai_handler = SegmentsAIHandler()\n",
        "TRAIN_DATASET_NAME = 'etaylor/train_cannabis_patches_dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bPRHcMrLm7l"
      },
      "source": [
        "#### 1. Upload your images and label a small subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20NDp5ubdoX5"
      },
      "source": [
        "##### Vizualize dataset images with masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FhD9WUM0Vv40",
        "outputId": "d76e231a-e4df-4e0e-b63d-6eed4dfc7f8c"
      },
      "outputs": [],
      "source": [
        "# Visualize the dataset\n",
        "# segmentsai_handler.visualize_dataset(TRAIN_DATASET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rotyn89DLm7m"
      },
      "source": [
        "#### 2. Train a segmentation model on the labeled images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWWJKe0YfarR",
        "outputId": "0bc3cbea-c69c-4fc0-9c25-b6293ae5f760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing dataset...\n",
            "Preloading all samples. This may take a while...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[38;2;255;153;0m██████████\u001b[0m| 31/31 [00:02<00:00, 10.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized dataset with 31 images.\n",
            "Exporting dataset. This may take a while...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[38;2;255;153;0m██████████\u001b[0m| 31/31 [00:00<00:00, 34.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported to checkpoints/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/17-11-2023_19-11-59/export_coco-instance_etaylor_train_cannabis_patches_dataset_v0.2.json. Images in segments/etaylor_train_cannabis_patches_dataset/v0.2\n",
            "Metadata(name='etaylor/train_cannabis_patches_dataset', json_file='checkpoints/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/17-11-2023_19-11-59/export_coco-instance_etaylor_train_cannabis_patches_dataset_v0.2.json', image_root='segments/etaylor_train_cannabis_patches_dataset/v0.2', evaluator_type='coco', thing_classes=['trichome', 'clear', 'cloudy', 'amber'])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Setup config for detectron2 model\n",
        "model_config = {\n",
        "'task_type': 'COCO-InstanceSegmentation',\n",
        "'model_type': 'mask_rcnn_R_50_FPN_3x',\n",
        "'dataset_name': TRAIN_DATASET_NAME,\n",
        "'release_version': \"v0.2\",\n",
        "'export_format': \"coco-instance\",\n",
        "'model_device': \"cuda\",\n",
        "}\n",
        "\n",
        "model = Detectron2Handler(**model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[11/17 19:12:11 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[11/17 19:12:11 d2.data.datasets.coco]: \u001b[0mLoaded 31 images in COCO format from checkpoints/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/17-11-2023_19-11-59/export_coco-instance_etaylor_train_cannabis_patches_dataset_v0.2.json\n",
            "\u001b[32m[11/17 19:12:11 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 31 images left.\n",
            "\u001b[32m[11/17 19:12:11 d2.data.build]: \u001b[0mDistribution of instances among all 4 categories:\n",
            "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|  trichome  | 543          |   clear    | 0            |   cloudy   | 0            |\n",
            "|   amber    | 0            |            |              |            |              |\n",
            "|   total    | 543          |            |              |            |              |\u001b[0m\n",
            "\u001b[32m[11/17 19:12:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[11/17 19:12:11 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[11/17 19:12:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[11/17 19:12:11 d2.data.common]: \u001b[0mSerializing 31 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[11/17 19:12:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.08 MiB\n",
            "\u001b[32m[11/17 19:12:11 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/17 19:12:11 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "\u001b[32m[11/17 19:12:11 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (5, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (16, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (16,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (4, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
            "Some model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[11/17 19:12:12 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/etaylor/.conda/envs/detectron/lib/python3.9/site-packages/detectron2/data/detection_utils.py:446: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n",
            "/home/etaylor/.conda/envs/detectron/lib/python3.9/site-packages/detectron2/data/detection_utils.py:446: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n",
            "/home/etaylor/.conda/envs/detectron/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[11/17 19:12:15 d2.utils.events]: \u001b[0m eta: 0:00:37  iter: 19  total_loss: 4.225  loss_cls: 1.669  loss_box_reg: 0.5449  loss_mask: 0.689  loss_rpn_cls: 1.185  loss_rpn_loc: 0.108    time: 0.1373  last_time: 0.1304  data_time: 0.0227  last_data_time: 0.0042   lr: 1.6068e-05  max_mem: 2004M\n",
            "\u001b[32m[11/17 19:12:18 d2.utils.events]: \u001b[0m eta: 0:00:35  iter: 39  total_loss: 2.854  loss_cls: 1.316  loss_box_reg: 0.6578  loss_mask: 0.6568  loss_rpn_cls: 0.2207  loss_rpn_loc: 0.069    time: 0.1413  last_time: 0.1527  data_time: 0.0130  last_data_time: 0.0131   lr: 3.2718e-05  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:21 d2.utils.events]: \u001b[0m eta: 0:00:32  iter: 59  total_loss: 2.39  loss_cls: 0.959  loss_box_reg: 0.6689  loss_mask: 0.5909  loss_rpn_cls: 0.1167  loss_rpn_loc: 0.07768    time: 0.1414  last_time: 0.1269  data_time: 0.0098  last_data_time: 0.0037   lr: 4.9367e-05  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:24 d2.utils.events]: \u001b[0m eta: 0:00:30  iter: 79  total_loss: 2.061  loss_cls: 0.6518  loss_box_reg: 0.7278  loss_mask: 0.5042  loss_rpn_cls: 0.08526  loss_rpn_loc: 0.06332    time: 0.1427  last_time: 0.1205  data_time: 0.0106  last_data_time: 0.0039   lr: 6.6017e-05  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:27 d2.utils.events]: \u001b[0m eta: 0:00:27  iter: 99  total_loss: 1.743  loss_cls: 0.4998  loss_box_reg: 0.7207  loss_mask: 0.407  loss_rpn_cls: 0.05851  loss_rpn_loc: 0.05752    time: 0.1421  last_time: 0.1230  data_time: 0.0071  last_data_time: 0.0039   lr: 8.2668e-05  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:29 d2.utils.events]: \u001b[0m eta: 0:00:24  iter: 119  total_loss: 1.559  loss_cls: 0.3943  loss_box_reg: 0.7102  loss_mask: 0.3553  loss_rpn_cls: 0.07152  loss_rpn_loc: 0.07021    time: 0.1410  last_time: 0.1439  data_time: 0.0106  last_data_time: 0.0134   lr: 9.9318e-05  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:32 d2.utils.events]: \u001b[0m eta: 0:00:22  iter: 139  total_loss: 1.42  loss_cls: 0.3298  loss_box_reg: 0.6657  loss_mask: 0.3192  loss_rpn_cls: 0.04157  loss_rpn_loc: 0.0547    time: 0.1414  last_time: 0.1352  data_time: 0.0136  last_data_time: 0.0038   lr: 0.00011597  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:35 d2.utils.events]: \u001b[0m eta: 0:00:19  iter: 159  total_loss: 1.272  loss_cls: 0.286  loss_box_reg: 0.6058  loss_mask: 0.2866  loss_rpn_cls: 0.03967  loss_rpn_loc: 0.05585    time: 0.1401  last_time: 0.1600  data_time: 0.0068  last_data_time: 0.0132   lr: 0.00013262  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:38 d2.utils.events]: \u001b[0m eta: 0:00:16  iter: 179  total_loss: 1.112  loss_cls: 0.2681  loss_box_reg: 0.4971  loss_mask: 0.2586  loss_rpn_cls: 0.0414  loss_rpn_loc: 0.05301    time: 0.1397  last_time: 0.1250  data_time: 0.0092  last_data_time: 0.0041   lr: 0.00014927  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:40 d2.utils.events]: \u001b[0m eta: 0:00:13  iter: 199  total_loss: 0.9546  loss_cls: 0.2249  loss_box_reg: 0.4377  loss_mask: 0.218  loss_rpn_cls: 0.03625  loss_rpn_loc: 0.05726    time: 0.1392  last_time: 0.1241  data_time: 0.0078  last_data_time: 0.0037   lr: 0.00016592  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:43 d2.utils.events]: \u001b[0m eta: 0:00:10  iter: 219  total_loss: 0.9089  loss_cls: 0.2127  loss_box_reg: 0.4065  loss_mask: 0.2264  loss_rpn_cls: 0.03344  loss_rpn_loc: 0.0528    time: 0.1385  last_time: 0.1215  data_time: 0.0066  last_data_time: 0.0040   lr: 0.00018257  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:46 d2.utils.events]: \u001b[0m eta: 0:00:08  iter: 239  total_loss: 0.9371  loss_cls: 0.2359  loss_box_reg: 0.4078  loss_mask: 0.2058  loss_rpn_cls: 0.03157  loss_rpn_loc: 0.05903    time: 0.1386  last_time: 0.1369  data_time: 0.0114  last_data_time: 0.0092   lr: 0.00019922  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:49 d2.utils.events]: \u001b[0m eta: 0:00:05  iter: 259  total_loss: 0.8372  loss_cls: 0.2037  loss_box_reg: 0.3669  loss_mask: 0.1841  loss_rpn_cls: 0.02395  loss_rpn_loc: 0.04495    time: 0.1383  last_time: 0.1317  data_time: 0.0075  last_data_time: 0.0039   lr: 0.00021587  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:51 d2.utils.events]: \u001b[0m eta: 0:00:02  iter: 279  total_loss: 0.7931  loss_cls: 0.1752  loss_box_reg: 0.3609  loss_mask: 0.1942  loss_rpn_cls: 0.02023  loss_rpn_loc: 0.05246    time: 0.1379  last_time: 0.1569  data_time: 0.0057  last_data_time: 0.0116   lr: 0.00023252  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:56 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 299  total_loss: 0.7434  loss_cls: 0.1733  loss_box_reg: 0.3176  loss_mask: 0.1775  loss_rpn_cls: 0.02347  loss_rpn_loc: 0.04847    time: 0.1380  last_time: 0.1321  data_time: 0.0117  last_data_time: 0.0036   lr: 0.00024917  max_mem: 2013M\n",
            "\u001b[32m[11/17 19:12:56 d2.engine.hooks]: \u001b[0mOverall training speed: 298 iterations in 0:00:41 (0.1380 s / it)\n",
            "\u001b[32m[11/17 19:12:56 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:43 (0:00:02 on hooks)\n",
            "\u001b[32m[11/17 19:12:56 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from checkpoints/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/17-11-2023_19-11-59/model_final.pth ...\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model.setup_training()\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8MsoYDLduR8"
      },
      "source": [
        "#### Create a new dataset and upload images to him"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK8SfINidxzP",
        "outputId": "3438e95d-4c34-4fb5-e573-5069fd59d4bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='cannabis_patches_week9_15_06_2023_3x_regular_IMG_2277' full_name='etaylor/cannabis_patches_week9_15_06_2023_3x_regular_IMG_2277' cloned_from=None description='cannabis patches week=week9_15_06_2023 zoom_type=3x_regular of image=IMG_2277.' category='other' public=False owner=Owner(username='etaylor', created_at='2022-12-28T12:53:18Z', email=None) created_at='2023-11-17T17:58:48.399583Z' enable_ratings=False enable_skip_labeling=True enable_skip_reviewing=False enable_save_button=False enable_label_status_verified=False enable_same_dimensions_track_constraint=False enable_interpolation=True task_type='segmentation-bitmap' label_stats=LabelStats(REVIEWED=None, REVIEWING_IN_PROGRESS=None, LABELED=None, LABELING_IN_PROGRESS=None, REJECTED=None, PRELABELED=None, SKIPPED=None, VERIFIED=None, UNLABELED=None, TOTAL=None) labeling_inactivity_timeout_seconds=None samples_count=0 collaborators_count=None task_attributes=TaskAttributes(format_version='0.1', categories=[TaskAttributeCategory(name='trichome', id=1, color=None, has_instances=None, attributes=None, dimensions=None), TaskAttributeCategory(name='clear', id=2, color=None, has_instances=None, attributes=None, dimensions=None), TaskAttributeCategory(name='cloudy', id=3, color=None, has_instances=None, attributes=None, dimensions=None), TaskAttributeCategory(name='amber', id=4, color=None, has_instances=None, attributes=None, dimensions=None)], image_attributes=None) labelsets=None role=None readme='' metadata={} noncollaborator_can_label=False noncollaborator_can_review=False insights_urls=None embeddings_enabled=None\n",
            "Uploaded IMG_2277.JPG and added as sample: uuid='16a63bbd-6d72-41fb-b6ce-fa4010c3a468' name='IMG_2277.JPG' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/fea194ae-61ce-4f58-93a1-09346c696586.jpg')) metadata={} created_at='2023-11-17T17:58:49.930210Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p0.png and added as sample: uuid='6d2f21b0-b597-436c-94b1-94c87c641047' name='IMG_2277_p0.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/1747f321-64f4-4941-ae1e-992f009214c9.png')) metadata={} created_at='2023-11-17T17:58:50.419761Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p1.png and added as sample: uuid='ef81beaa-339b-4f37-b724-ae22db3a1492' name='IMG_2277_p1.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/e92a8dc7-88ca-4c69-8276-46b67163e169.png')) metadata={} created_at='2023-11-17T17:58:50.897955Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p2.png and added as sample: uuid='b7a35981-194d-435c-8d13-d73fe04c7996' name='IMG_2277_p2.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/848f911b-80cf-437c-9753-d799313e5c04.png')) metadata={} created_at='2023-11-17T17:58:51.448277Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p3.png and added as sample: uuid='6ac0d218-a284-496a-af21-bbdb81ff6d70' name='IMG_2277_p3.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/36ef8076-f3d0-4fc7-9db2-8e8c63f8b484.png')) metadata={} created_at='2023-11-17T17:58:51.920124Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p4.png and added as sample: uuid='51270d8d-4e30-41ca-8b00-118fe300efe2' name='IMG_2277_p4.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/6398d180-1564-4c52-89f2-76cc64f66839.png')) metadata={} created_at='2023-11-17T17:58:52.460725Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p5.png and added as sample: uuid='bca8bbaa-ff94-4d49-8eb5-c8eb8bafc5aa' name='IMG_2277_p5.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/b3b1005f-e698-4b9a-8283-07d8c8e1be07.png')) metadata={} created_at='2023-11-17T17:58:53.026865Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p6.png and added as sample: uuid='21566954-e615-415d-a103-44ae0d545857' name='IMG_2277_p6.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/62eaf17b-2450-480b-95dc-470f49f86bec.png')) metadata={} created_at='2023-11-17T17:58:53.536655Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p7.png and added as sample: uuid='8de976ab-5a19-483a-95ea-f9ce87a76bc2' name='IMG_2277_p7.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/e3843d81-c3f0-497d-ade7-63b1cf6a8444.png')) metadata={} created_at='2023-11-17T17:58:54.069306Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p8.png and added as sample: uuid='9459b77c-6bed-4eb7-bdcd-3a88a6ee17b8' name='IMG_2277_p8.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/13f5d989-5ec4-4fd5-88f1-5205e270ee82.png')) metadata={} created_at='2023-11-17T17:58:54.580283Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p9.png and added as sample: uuid='a5578d56-bb86-47c8-9963-0feb029c7487' name='IMG_2277_p9.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/de2f6a40-de8f-4b43-bb5d-4651db813264.png')) metadata={} created_at='2023-11-17T17:58:55.111975Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p10.png and added as sample: uuid='159775e4-20e6-424d-9c29-7b31dd0239b9' name='IMG_2277_p10.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/08c59bde-2458-4ffb-9a5f-01f98d2819a4.png')) metadata={} created_at='2023-11-17T17:58:55.583890Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p11.png and added as sample: uuid='a7723bdf-47d6-48d3-af7a-4e2358aac685' name='IMG_2277_p11.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/f5ea59fe-bf7f-458b-8b84-626e8e633322.png')) metadata={} created_at='2023-11-17T17:58:56.123389Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p12.png and added as sample: uuid='2c001a6a-2257-4c31-8be6-1ed2e74b01fc' name='IMG_2277_p12.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/d60eda66-e5c3-4252-91b9-cbec80b25f38.png')) metadata={} created_at='2023-11-17T17:58:56.630722Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p13.png and added as sample: uuid='539fc0e3-7600-46e8-9d94-3a5bbd67dcf4' name='IMG_2277_p13.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/e0a0e29c-21f0-43dd-9787-0e5c32966970.png')) metadata={} created_at='2023-11-17T17:58:57.175731Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2277_p14.png and added as sample: uuid='28f33dbf-4635-4a8c-97e4-a9b4a9f6da25' name='IMG_2277_p14.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/5c1c7852-53e2-4ad2-9674-404e89b7c298.png')) metadata={} created_at='2023-11-17T17:58:57.725929Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n"
          ]
        }
      ],
      "source": [
        "# New dataset to upload variables\n",
        "image_name = \"IMG_2277\"\n",
        "week = 'week9'\n",
        "zoom_type = '3xr'\n",
        "\n",
        "# Create a new test dataset for the specified image, week, and zoom type\n",
        "test_dataset = pipe.create_new_test_dataset(image_name, config.WEEKS_DIR[week], config.ZOOM_TYPES_DIR[zoom_type], single_category=False)\n",
        "# # Get the absolute path to the processed image\n",
        "abs_images_path = f\"{config.get_processed_cannabis_image_path(week, zoom_type)}/{image_name}\"\n",
        "raw_images_path = f\"{config.get_raw_image_path(week, zoom_type)}/{image_name}.JPG\"\n",
        "\n",
        "# Upload the images that are not annotated to the dataset\n",
        "segmentsai_handler.upload_single_image(test_dataset, raw_images_path) # upload the raw image path\n",
        "segmentsai_handler.upload_images(test_dataset, abs_images_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting 5 seconds that the release is created...\n",
            "uuid='0ab0c0e8-5d8c-45c5-a633-97722cd13582' name='v0.1' description='upload predictions to dataset.' release_type='JSON' attributes=URL(url='https://segmentsai-prod.s3.amazonaws.com/releases/0ab0c0e8-5d8c-45c5-a633-97722cd13582.json?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5RYRXRX22Y5Q2VMR%2F20231117%2Feu-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231117T175910Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=3e1e2f6e2b9ab7a08ff9a0af488afbdb77a6e9b61f32bbb9c119b6219a3fccb7') status='SUCCEEDED' created_at='2023-11-17T17:59:05.521175Z' samples_count=16\n"
          ]
        }
      ],
      "source": [
        "release_name = \"v0.1\"\n",
        "description = \"upload predictions to dataset.\"\n",
        "segmentsai_handler.client.add_release(test_dataset, release_name, description)\n",
        "print(\"Waiting 5 seconds that the release is created...\")\n",
        "time.sleep(5)\n",
        "test_release = segmentsai_handler.client.get_release(test_dataset, \"v0.1\")\n",
        "print(test_release)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional - Load a model checkpoint without training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_path = \"\"\n",
        "model.load_checkpoint(checkpoint_path=checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpX-71UwlW6o"
      },
      "source": [
        "### Upload the Images that are not annotated to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gozhWa6Ahvxm",
        "outputId": "f73115aa-9519-491c-ac9d-8da8a6b8b70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing dataset...\n",
            "Preloading all samples. This may take a while...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[38;2;255;153;0m██████████\u001b[0m| 16/16 [00:01<00:00,  8.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized dataset with 16 images.\n",
            "(3024, 4032, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n",
            "(512, 512, 3)\n"
          ]
        }
      ],
      "source": [
        "test_dataset_instance = SegmentsDataset(test_release)\n",
        "\n",
        "\n",
        "for sample in test_dataset_instance:\n",
        "    # Generate label predictions\n",
        "    image = np.array(sample[\"image\"])\n",
        "    segmentation_bitmap, annotations = model.predict_image(image)\n",
        "    segmentsai_handler.upload_annotation_for_sample(sample['uuid'], segmentation_bitmap, annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the dataset - ONLY IF YOU WANT TO DELETE THE DATASET!!!\n",
        "DELETE_DATASET_NAME = \"\"\n",
        "segmentsai_handler.client.delete_dataset(DELETE_DATASET_NAME)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
