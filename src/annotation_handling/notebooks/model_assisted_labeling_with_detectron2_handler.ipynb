{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wnygc-uoVLfx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/etaylor/.conda/envs/detectron/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from src.annotation_handling.segmentsai_handler import SegmentsAIHandler\n",
        "from src.pipelines import model_assist_label_pipeline as pipe \n",
        "from src.segmentation.framework_handlers.detectron2_handler import Detectron2Handler\n",
        "import config\n",
        "\n",
        "from segments import SegmentsDataset\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw2JEh1Sk52I"
      },
      "source": [
        "#### Setup Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zgjm9kb2M9lL"
      },
      "outputs": [],
      "source": [
        "segmentsai_handler = SegmentsAIHandler()\n",
        "TRAIN_DATASET_NAME = 'etaylor/cannabis_patches_all_images'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bPRHcMrLm7l"
      },
      "source": [
        "#### 1. Upload your images and label a small subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20NDp5ubdoX5"
      },
      "source": [
        "##### Vizualize dataset images with masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FhD9WUM0Vv40",
        "outputId": "d76e231a-e4df-4e0e-b63d-6eed4dfc7f8c"
      },
      "outputs": [],
      "source": [
        "# Visualize the dataset\n",
        "segmentsai_handler.visualize_dataset(TRAIN_DATASET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rotyn89DLm7m"
      },
      "source": [
        "#### 2. Train a segmentation model on the labeled images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWWJKe0YfarR",
        "outputId": "0bc3cbea-c69c-4fc0-9c25-b6293ae5f760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing dataset...\n",
            "Preloading all samples. This may take a while...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[38;2;255;153;0m██████████\u001b[0m| 95/95 [00:03<00:00, 30.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized dataset with 95 images.\n",
            "Exporting dataset. This may take a while...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|\u001b[38;2;255;153;0m████████  \u001b[0m| 76/95 [00:01<00:00, 63.34it/s]Skipping instance with 0 labeled pixels: IMG_2305_p7.jpg, instance_id: 3, category_id: 1\n",
            "100%|\u001b[38;2;255;153;0m██████████\u001b[0m| 95/95 [00:01<00:00, 58.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported to checkpoints/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/15-11-2023_16-41-44/export_coco-instance_etaylor_cannabis_patches_all_images_v0.2.json. Images in checkpoints/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/15-11-2023_16-41-44/segments/etaylor_cannabis_patches_all_images/v0.2\n",
            "Metadata(name='etaylor/cannabis_patches_all_images', json_file='checkpoints/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/15-11-2023_16-41-44/export_coco-instance_etaylor_cannabis_patches_all_images_v0.2.json', image_root='checkpoints/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/15-11-2023_16-41-44/segments/etaylor_cannabis_patches_all_images/v0.2', evaluator_type='coco', thing_classes=['trichome'])\n",
            "\u001b[32m[11/15 16:41:54 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[11/15 16:41:54 d2.data.datasets.coco]: \u001b[0mLoaded 92 images in COCO format from checkpoints/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/15-11-2023_16-41-44/export_coco-instance_etaylor_cannabis_patches_all_images_v0.2.json\n",
            "\u001b[32m[11/15 16:41:54 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 92 images left.\n",
            "\u001b[32m[11/15 16:41:54 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
            "\u001b[36m|  category  | #instances   |\n",
            "|:----------:|:-------------|\n",
            "|  trichome  | 2185         |\n",
            "|            |              |\u001b[0m\n",
            "\u001b[32m[11/15 16:41:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[11/15 16:41:54 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[11/15 16:41:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[11/15 16:41:54 d2.data.common]: \u001b[0mSerializing 92 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[11/15 16:41:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.32 MiB\n",
            "\u001b[32m[11/15 16:41:54 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/15 16:41:54 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "\u001b[32m[11/15 16:41:54 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
            "Some model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[11/15 16:41:54 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/etaylor/.conda/envs/detectron/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/home/etaylor/.conda/envs/detectron/lib/python3.9/site-packages/detectron2/data/detection_utils.py:446: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n",
            "/home/etaylor/.conda/envs/detectron/lib/python3.9/site-packages/detectron2/data/detection_utils.py:446: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[11/15 16:41:58 d2.utils.events]: \u001b[0m eta: 0:00:51  iter: 19  total_loss: 3.723  loss_cls: 0.6848  loss_box_reg: 0.641  loss_mask: 0.6822  loss_rpn_cls: 1.614  loss_rpn_loc: 0.1372    time: 0.1856  last_time: 0.1786  data_time: 0.0168  last_data_time: 0.0045   lr: 1.6068e-05  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:02 d2.utils.events]: \u001b[0m eta: 0:00:48  iter: 39  total_loss: 2.458  loss_cls: 0.6131  loss_box_reg: 0.6992  loss_mask: 0.6515  loss_rpn_cls: 0.3468  loss_rpn_loc: 0.1024    time: 0.1868  last_time: 0.1766  data_time: 0.0053  last_data_time: 0.0038   lr: 3.2718e-05  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:06 d2.utils.events]: \u001b[0m eta: 0:00:44  iter: 59  total_loss: 2.063  loss_cls: 0.5256  loss_box_reg: 0.707  loss_mask: 0.5777  loss_rpn_cls: 0.1028  loss_rpn_loc: 0.08791    time: 0.1846  last_time: 0.1673  data_time: 0.0054  last_data_time: 0.0040   lr: 4.9367e-05  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:10 d2.utils.events]: \u001b[0m eta: 0:00:40  iter: 79  total_loss: 1.909  loss_cls: 0.4794  loss_box_reg: 0.7353  loss_mask: 0.4942  loss_rpn_cls: 0.09682  loss_rpn_loc: 0.07743    time: 0.1834  last_time: 0.1877  data_time: 0.0065  last_data_time: 0.0042   lr: 6.6017e-05  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:13 d2.utils.events]: \u001b[0m eta: 0:00:37  iter: 99  total_loss: 1.786  loss_cls: 0.439  loss_box_reg: 0.748  loss_mask: 0.4276  loss_rpn_cls: 0.07589  loss_rpn_loc: 0.0719    time: 0.1844  last_time: 0.1914  data_time: 0.0063  last_data_time: 0.0129   lr: 8.2668e-05  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:17 d2.utils.events]: \u001b[0m eta: 0:00:33  iter: 119  total_loss: 1.647  loss_cls: 0.3868  loss_box_reg: 0.7  loss_mask: 0.3711  loss_rpn_cls: 0.06746  loss_rpn_loc: 0.09164    time: 0.1847  last_time: 0.1605  data_time: 0.0045  last_data_time: 0.0044   lr: 9.9318e-05  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:21 d2.utils.events]: \u001b[0m eta: 0:00:29  iter: 139  total_loss: 1.578  loss_cls: 0.3721  loss_box_reg: 0.6788  loss_mask: 0.3526  loss_rpn_cls: 0.06688  loss_rpn_loc: 0.07628    time: 0.1851  last_time: 0.1754  data_time: 0.0056  last_data_time: 0.0046   lr: 0.00011597  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:25 d2.utils.events]: \u001b[0m eta: 0:00:26  iter: 159  total_loss: 1.396  loss_cls: 0.3083  loss_box_reg: 0.6148  loss_mask: 0.3093  loss_rpn_cls: 0.04731  loss_rpn_loc: 0.08143    time: 0.1853  last_time: 0.1994  data_time: 0.0068  last_data_time: 0.0075   lr: 0.00013262  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:28 d2.utils.events]: \u001b[0m eta: 0:00:22  iter: 179  total_loss: 1.247  loss_cls: 0.282  loss_box_reg: 0.5727  loss_mask: 0.246  loss_rpn_cls: 0.06372  loss_rpn_loc: 0.07498    time: 0.1854  last_time: 0.2020  data_time: 0.0064  last_data_time: 0.0036   lr: 0.00014927  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:32 d2.utils.events]: \u001b[0m eta: 0:00:18  iter: 199  total_loss: 1.129  loss_cls: 0.2573  loss_box_reg: 0.4733  loss_mask: 0.2184  loss_rpn_cls: 0.05355  loss_rpn_loc: 0.07785    time: 0.1865  last_time: 0.2105  data_time: 0.0086  last_data_time: 0.0145   lr: 0.00016592  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:36 d2.utils.events]: \u001b[0m eta: 0:00:14  iter: 219  total_loss: 0.9793  loss_cls: 0.2335  loss_box_reg: 0.4295  loss_mask: 0.2024  loss_rpn_cls: 0.045  loss_rpn_loc: 0.08192    time: 0.1858  last_time: 0.1737  data_time: 0.0053  last_data_time: 0.0053   lr: 0.00018257  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:40 d2.utils.events]: \u001b[0m eta: 0:00:11  iter: 239  total_loss: 0.8973  loss_cls: 0.2384  loss_box_reg: 0.3504  loss_mask: 0.1688  loss_rpn_cls: 0.05524  loss_rpn_loc: 0.07372    time: 0.1857  last_time: 0.1855  data_time: 0.0067  last_data_time: 0.0028   lr: 0.00019922  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:43 d2.utils.events]: \u001b[0m eta: 0:00:07  iter: 259  total_loss: 0.8231  loss_cls: 0.1992  loss_box_reg: 0.3528  loss_mask: 0.166  loss_rpn_cls: 0.03245  loss_rpn_loc: 0.07026    time: 0.1854  last_time: 0.1704  data_time: 0.0058  last_data_time: 0.0056   lr: 0.00021587  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:47 d2.utils.events]: \u001b[0m eta: 0:00:03  iter: 279  total_loss: 0.8078  loss_cls: 0.2335  loss_box_reg: 0.2988  loss_mask: 0.1623  loss_rpn_cls: 0.04117  loss_rpn_loc: 0.07903    time: 0.1854  last_time: 0.1915  data_time: 0.0052  last_data_time: 0.0033   lr: 0.00023252  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:54 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 299  total_loss: 0.8792  loss_cls: 0.1997  loss_box_reg: 0.3287  loss_mask: 0.1699  loss_rpn_cls: 0.04387  loss_rpn_loc: 0.07391    time: 0.1852  last_time: 0.1628  data_time: 0.0054  last_data_time: 0.0042   lr: 0.00024917  max_mem: 2026M\n",
            "\u001b[32m[11/15 16:42:54 d2.engine.hooks]: \u001b[0mOverall training speed: 298 iterations in 0:00:55 (0.1852 s / it)\n",
            "\u001b[32m[11/15 16:42:54 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:59 (0:00:04 on hooks)\n",
            "\u001b[32m[11/15 16:42:55 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from checkpoints/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/15-11-2023_16-41-44/model_final.pth ...\n"
          ]
        }
      ],
      "source": [
        "# Setup config for detectron2 model\n",
        "model_config = {\n",
        "'task_type': 'COCO-InstanceSegmentation',\n",
        "'model_type': 'mask_rcnn_R_50_FPN_3x',\n",
        "'dataset_name': TRAIN_DATASET_NAME,\n",
        "'release_version': \"v0.2\",\n",
        "'export_format': \"coco-instance\",\n",
        "'model_device': \"cuda\",\n",
        "}\n",
        "\n",
        "model = Detectron2Handler(**model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.setup_training()\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8MsoYDLduR8"
      },
      "source": [
        "#### Create a new dataset and upload images to him"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK8SfINidxzP",
        "outputId": "3438e95d-4c34-4fb5-e573-5069fd59d4bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='cannabis_patches_week9_15_06_2023_3x_regular_IMG_2242' full_name='etaylor/cannabis_patches_week9_15_06_2023_3x_regular_IMG_2242' cloned_from=None description='cannabis patches week=week9_15_06_2023 zoom_type=3x_regular of image=IMG_2242.' category='other' public=False owner=Owner(username='etaylor', created_at='2022-12-28T12:53:18Z', email=None) created_at='2023-11-15T14:43:37.874110Z' enable_ratings=False enable_skip_labeling=True enable_skip_reviewing=False enable_save_button=False enable_label_status_verified=False enable_same_dimensions_track_constraint=False enable_interpolation=True task_type='segmentation-bitmap' label_stats=LabelStats(REVIEWED=None, REVIEWING_IN_PROGRESS=None, LABELED=None, LABELING_IN_PROGRESS=None, REJECTED=None, PRELABELED=None, SKIPPED=None, VERIFIED=None, UNLABELED=None, TOTAL=None) labeling_inactivity_timeout_seconds=None samples_count=0 collaborators_count=None task_attributes=TaskAttributes(format_version='0.1', categories=[TaskAttributeCategory(name='trichome', id=1, color=None, has_instances=None, attributes=None, dimensions=None)], image_attributes=None) labelsets=None role=None readme='' metadata={} noncollaborator_can_label=False noncollaborator_can_review=False insights_urls=None embeddings_enabled=None\n",
            "Uploaded IMG_2242_p0.png and added as sample: uuid='3f028fe9-bed7-45ce-b4e7-96406828933a' name='IMG_2242_p0.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/1bc495f5-f236-4585-b9a0-282501fda1d0.png')) metadata={} created_at='2023-11-15T14:43:39.071658Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p1.png and added as sample: uuid='edd78dfb-1df6-4904-84be-663d58b14370' name='IMG_2242_p1.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/434fb321-f79a-4486-aa62-ade44cd1c905.png')) metadata={} created_at='2023-11-15T14:43:39.609639Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p2.png and added as sample: uuid='0bfb6de5-eaa9-4cd6-8448-b3f28d36e6e1' name='IMG_2242_p2.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/a6c40f01-b7d2-4283-aa27-966bd043b63a.png')) metadata={} created_at='2023-11-15T14:43:40.110184Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p3.png and added as sample: uuid='23edc2ae-f974-46bd-8bd0-325ee5838ac2' name='IMG_2242_p3.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/9fdf1414-c35f-4e43-bc53-b5ad61366166.png')) metadata={} created_at='2023-11-15T14:43:40.602212Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p4.png and added as sample: uuid='11941182-741d-4fc7-8f77-6197a16a82d0' name='IMG_2242_p4.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/924ce64e-a756-4837-9b8b-adeb9bf1c683.png')) metadata={} created_at='2023-11-15T14:43:41.065783Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p5.png and added as sample: uuid='2c4037dc-7168-43b6-b136-410252237c7d' name='IMG_2242_p5.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/caa6742b-ca51-44ed-8fcc-41b064304c2f.png')) metadata={} created_at='2023-11-15T14:43:41.544423Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p6.png and added as sample: uuid='e6b1d23d-23b5-4193-b7f3-21490b599979' name='IMG_2242_p6.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/7cd9f151-7133-4b16-ad41-85814608b6ae.png')) metadata={} created_at='2023-11-15T14:43:42.045776Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p7.png and added as sample: uuid='ea74f97d-07ff-4dec-9be4-7a71a75b97f1' name='IMG_2242_p7.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/e97a0adc-e59b-4e99-b0c1-208da4f8d44a.png')) metadata={} created_at='2023-11-15T14:43:42.540053Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p8.png and added as sample: uuid='4468a6f0-1ef0-474b-98e2-c98bd8dc6729' name='IMG_2242_p8.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/d15085a2-2f89-45e0-99bb-5ba953608b24.png')) metadata={} created_at='2023-11-15T14:43:43.006950Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p9.png and added as sample: uuid='b0632a1a-9936-47b3-88f1-332102d2f1f1' name='IMG_2242_p9.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/56061b9d-f8c1-4a02-bc96-241a999cfa94.png')) metadata={} created_at='2023-11-15T14:43:43.513147Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p10.png and added as sample: uuid='d4bb0edf-b40c-4915-aeaa-47ac203047f5' name='IMG_2242_p10.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/e2c463bd-59b4-4860-bec4-a1be91020593.png')) metadata={} created_at='2023-11-15T14:43:43.951259Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p11.png and added as sample: uuid='dbd4ec40-83a7-4b35-b945-0a2a98eada63' name='IMG_2242_p11.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/23e19508-9dbe-4387-8a5e-62879da16ee9.png')) metadata={} created_at='2023-11-15T14:43:44.420337Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p12.png and added as sample: uuid='b3ff7c0c-9b87-455d-be94-f7e0f592c57f' name='IMG_2242_p12.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/4b58d228-de39-4b69-8d02-96a2c718ba17.png')) metadata={} created_at='2023-11-15T14:43:44.789714Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p13.png and added as sample: uuid='c410d5f0-50d5-496c-8b34-1bec33b76bbc' name='IMG_2242_p13.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/7d386a29-7cc5-43a6-8d36-b961be6b4613.png')) metadata={} created_at='2023-11-15T14:43:45.318132Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p14.png and added as sample: uuid='1162c728-9317-42b7-ba9b-268ae098aa93' name='IMG_2242_p14.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/45cede8b-26eb-4ba8-9e0e-509f4d3fdf15.png')) metadata={} created_at='2023-11-15T14:43:45.932361Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2242_p15.png and added as sample: uuid='1f198b6d-5fca-449c-a78e-ca04e2e0b6af' name='IMG_2242_p15.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/a88cd93b-5e52-4e7d-8c3b-ebbe9e31f2a7.png')) metadata={} created_at='2023-11-15T14:43:46.437164Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n"
          ]
        }
      ],
      "source": [
        "# New dataset to upload variables\n",
        "image_name = \"IMG_2242\"\n",
        "week = 'week9'\n",
        "zoom_type = '3xr'\n",
        "\n",
        "# Create a new test dataset for the specified image, week, and zoom type\n",
        "test_dataset = pipe.create_new_test_dataset(image_name, config.WEEKS_DIR[week], config.ZOOM_TYPES_DIR[zoom_type])\n",
        "# Get the absolute path to the processed image\n",
        "abs_images_path = f\"{config.get_processed_cannabis_image_path(week, zoom_type)}/{image_name}\"\n",
        "\n",
        "# Upload the images that are not annotated to the dataset\n",
        "segmentsai_handler.upload_images(test_dataset, abs_images_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Release(uuid='ff5fd20f-4fb7-4cfc-8f1d-4d9b1b6ca9df', name='v0.1', description='upload predictions to dataset.', release_type='JSON', attributes=URL(url=''), status='PENDING', created_at='2023-11-15T14:44:01.546601Z', samples_count=16)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "release_name = \"v0.1\"\n",
        "description = \"upload predictions to dataset.\"\n",
        "segmentsai_handler.client.add_release(test_dataset, release_name, description)\n",
        "time.sleep(5)\n",
        "print(\"Waiting 5 seconds that the release is created...\")\n",
        "test_release = segmentsai_handler.client.get_release(test_dataset, \"v0.1\")\n",
        "print(test_release)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional - Load a model checkpoint without training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_path = \"\"\n",
        "model.load_checkpoint(checkpoint_path=checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpX-71UwlW6o"
      },
      "source": [
        "### Upload the Images that are not annotated to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gozhWa6Ahvxm",
        "outputId": "f73115aa-9519-491c-ac9d-8da8a6b8b70b"
      },
      "outputs": [],
      "source": [
        "test_dataset_instance = SegmentsDataset(test_release)\n",
        "\n",
        "\n",
        "for sample in test_dataset_instance:\n",
        "    # Generate label predictions\n",
        "    image = np.array(sample[\"image\"])\n",
        "    segmentation_bitmap, annotations = model.predict_image(image)\n",
        "    segmentsai_handler.upload_annotation_for_sample(sample['uuid'], segmentation_bitmap, annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the dataset - ONLY IF YOU WANT TO DELETE THE DATASET!!!\n",
        "DELETE_DATASET_NAME = \"\"\n",
        "segmentsai_handler.client.delete_dataset(DELETE_DATASET_NAME)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
