{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnygc-uoVLfx"
      },
      "outputs": [],
      "source": [
        "from src.annotation_handling.segmentsai_handler import SegmentsAIHandler\n",
        "from src.pipelines import model_assist_label_pipeline as pipe \n",
        "import config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw2JEh1Sk52I"
      },
      "source": [
        "#### Setup Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zgjm9kb2M9lL"
      },
      "outputs": [],
      "source": [
        "segmentsai_handler = SegmentsAIHandler()\n",
        "TRAIN_DATASET_NAME = 'etaylor/cannabis_patches_all_images'\n",
        "\n",
        "# New dataset to upload variables\n",
        "image_name = \"IMG_2288\"\n",
        "week = 'week9'\n",
        "zoom_type = '3xr'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bPRHcMrLm7l"
      },
      "source": [
        "#### 1. Upload your images and label a small subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20NDp5ubdoX5"
      },
      "source": [
        "##### Vizualize dataset images with masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FhD9WUM0Vv40",
        "outputId": "d76e231a-e4df-4e0e-b63d-6eed4dfc7f8c"
      },
      "outputs": [],
      "source": [
        "# Visualize the dataset\n",
        "segmentsai_handler.visualize_dataset(TRAIN_DATASET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rotyn89DLm7m"
      },
      "source": [
        "#### 2. Train a segmentation model on the labeled images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWWJKe0YfarR",
        "outputId": "0bc3cbea-c69c-4fc0-9c25-b6293ae5f760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing dataset...\n",
            "Preloading all samples. This may take a while...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[38;2;255;153;0m██████████\u001b[0m| 95/95 [00:03<00:00, 29.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized dataset with 95 images.\n",
            "Exporting dataset. This may take a while...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|\u001b[38;2;255;153;0m███████▉  \u001b[0m| 75/95 [00:01<00:00, 59.81it/s]Skipping instance with 0 labeled pixels: IMG_2305_p7.jpg, instance_id: 3, category_id: 1\n",
            "100%|\u001b[38;2;255;153;0m██████████\u001b[0m| 95/95 [00:01<00:00, 55.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported to ./export_coco-instance_etaylor_cannabis_patches_all_images_v0.2.json. Images in segments/etaylor_cannabis_patches_all_images/v0.2\n",
            "Dataset was already registered\n",
            "\u001b[32m[11/09 13:06:16 d2.data.datasets.coco]: \u001b[0mLoaded 92 images in COCO format from ./export_coco-instance_etaylor_cannabis_patches_all_images_v0.2.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata(name='my_dataset', json_file='./export_coco-instance_etaylor_cannabis_patches_all_images_v0.2.json', image_root='segments/etaylor_cannabis_patches_all_images/v0.2', evaluator_type='coco', thing_classes=['trichome'], thing_dataset_id_to_contiguous_id={1: 0})\n",
            "/home/etaylor/pycharmProjects/thesis/extra_deps/detectron2/detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\n",
            "\u001b[32m[11/09 13:06:16 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[11/09 13:06:16 d2.data.datasets.coco]: \u001b[0mLoaded 92 images in COCO format from ./export_coco-instance_etaylor_cannabis_patches_all_images_v0.2.json\n",
            "\u001b[32m[11/09 13:06:16 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 92 images left.\n",
            "\u001b[32m[11/09 13:06:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[11/09 13:06:16 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[11/09 13:06:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[11/09 13:06:16 d2.data.common]: \u001b[0mSerializing 92 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[11/09 13:06:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.32 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/09 13:06:16 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "\u001b[32m[11/09 13:06:16 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
            "Some model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[11/09 13:06:17 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/etaylor/pycharmProjects/thesis/extra_deps/detectron2/detectron2/data/detection_utils.py:467: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n",
            "/home/etaylor/pycharmProjects/thesis/extra_deps/detectron2/detectron2/data/detection_utils.py:467: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[11/09 13:06:21 d2.utils.events]: \u001b[0m eta: 0:00:51  iter: 19  total_loss: 3.946  loss_cls: 0.72  loss_box_reg: 0.6022  loss_mask: 0.6961  loss_rpn_cls: 1.72  loss_rpn_loc: 0.1428    time: 0.1921  last_time: 0.1131  data_time: 0.0637  last_data_time: 0.0073   lr: 1.6068e-05  max_mem: 1985M\n",
            "\u001b[32m[11/09 13:06:25 d2.utils.events]: \u001b[0m eta: 0:00:45  iter: 39  total_loss: 2.499  loss_cls: 0.6242  loss_box_reg: 0.7492  loss_mask: 0.6638  loss_rpn_cls: 0.3667  loss_rpn_loc: 0.1039    time: 0.1881  last_time: 0.1351  data_time: 0.0351  last_data_time: 0.0065   lr: 3.2718e-05  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:06:29 d2.utils.events]: \u001b[0m eta: 0:00:43  iter: 59  total_loss: 2.08  loss_cls: 0.538  loss_box_reg: 0.7274  loss_mask: 0.597  loss_rpn_cls: 0.09522  loss_rpn_loc: 0.09426    time: 0.1910  last_time: 0.1629  data_time: 0.0343  last_data_time: 0.0359   lr: 4.9367e-05  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:06:33 d2.utils.events]: \u001b[0m eta: 0:00:39  iter: 79  total_loss: 1.941  loss_cls: 0.4836  loss_box_reg: 0.7251  loss_mask: 0.5102  loss_rpn_cls: 0.08417  loss_rpn_loc: 0.08767    time: 0.1955  last_time: 0.1616  data_time: 0.0437  last_data_time: 0.0098   lr: 6.6017e-05  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:06:37 d2.utils.events]: \u001b[0m eta: 0:00:36  iter: 99  total_loss: 1.769  loss_cls: 0.4404  loss_box_reg: 0.7446  loss_mask: 0.425  loss_rpn_cls: 0.08069  loss_rpn_loc: 0.08202    time: 0.1951  last_time: 0.1584  data_time: 0.0380  last_data_time: 0.0295   lr: 8.2668e-05  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:06:41 d2.utils.events]: \u001b[0m eta: 0:00:32  iter: 119  total_loss: 1.726  loss_cls: 0.4152  loss_box_reg: 0.6978  loss_mask: 0.3817  loss_rpn_cls: 0.07835  loss_rpn_loc: 0.08071    time: 0.1959  last_time: 0.4779  data_time: 0.0453  last_data_time: 0.3007   lr: 9.9318e-05  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:06:45 d2.utils.events]: \u001b[0m eta: 0:00:27  iter: 139  total_loss: 1.548  loss_cls: 0.3673  loss_box_reg: 0.6723  loss_mask: 0.3342  loss_rpn_cls: 0.06051  loss_rpn_loc: 0.07676    time: 0.1949  last_time: 0.2094  data_time: 0.0402  last_data_time: 0.0152   lr: 0.00011597  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:06:48 d2.utils.events]: \u001b[0m eta: 0:00:24  iter: 159  total_loss: 1.432  loss_cls: 0.333  loss_box_reg: 0.6537  loss_mask: 0.3183  loss_rpn_cls: 0.05563  loss_rpn_loc: 0.08164    time: 0.1941  last_time: 0.1545  data_time: 0.0313  last_data_time: 0.0170   lr: 0.00013262  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:06:52 d2.utils.events]: \u001b[0m eta: 0:00:21  iter: 179  total_loss: 1.216  loss_cls: 0.2901  loss_box_reg: 0.5665  loss_mask: 0.2451  loss_rpn_cls: 0.03749  loss_rpn_loc: 0.07549    time: 0.1945  last_time: 0.1479  data_time: 0.0484  last_data_time: 0.0383   lr: 0.00014927  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:06:56 d2.utils.events]: \u001b[0m eta: 0:00:17  iter: 199  total_loss: 1.093  loss_cls: 0.2474  loss_box_reg: 0.4781  loss_mask: 0.2249  loss_rpn_cls: 0.04496  loss_rpn_loc: 0.07494    time: 0.1931  last_time: 0.1262  data_time: 0.0276  last_data_time: 0.0103   lr: 0.00016592  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:07:00 d2.utils.events]: \u001b[0m eta: 0:00:14  iter: 219  total_loss: 1.029  loss_cls: 0.2567  loss_box_reg: 0.4139  loss_mask: 0.1967  loss_rpn_cls: 0.04827  loss_rpn_loc: 0.07577    time: 0.1935  last_time: 0.1413  data_time: 0.0330  last_data_time: 0.0110   lr: 0.00018257  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:07:04 d2.utils.events]: \u001b[0m eta: 0:00:10  iter: 239  total_loss: 0.904  loss_cls: 0.2462  loss_box_reg: 0.3747  loss_mask: 0.1753  loss_rpn_cls: 0.04493  loss_rpn_loc: 0.06182    time: 0.1924  last_time: 0.1955  data_time: 0.0248  last_data_time: 0.0076   lr: 0.00019922  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:07:07 d2.utils.events]: \u001b[0m eta: 0:00:07  iter: 259  total_loss: 0.9418  loss_cls: 0.2386  loss_box_reg: 0.3496  loss_mask: 0.1802  loss_rpn_cls: 0.05548  loss_rpn_loc: 0.0814    time: 0.1922  last_time: 0.1390  data_time: 0.0228  last_data_time: 0.0068   lr: 0.00021587  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:07:11 d2.utils.events]: \u001b[0m eta: 0:00:03  iter: 279  total_loss: 0.8043  loss_cls: 0.2146  loss_box_reg: 0.3105  loss_mask: 0.1584  loss_rpn_cls: 0.04293  loss_rpn_loc: 0.08324    time: 0.1918  last_time: 0.2020  data_time: 0.0309  last_data_time: 0.0140   lr: 0.00023252  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:07:16 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 299  total_loss: 0.784  loss_cls: 0.2093  loss_box_reg: 0.289  loss_mask: 0.1525  loss_rpn_cls: 0.03289  loss_rpn_loc: 0.05848    time: 0.1903  last_time: 0.1476  data_time: 0.0215  last_data_time: 0.0123   lr: 0.00024917  max_mem: 2001M\n",
            "\u001b[32m[11/09 13:07:16 d2.engine.hooks]: \u001b[0mOverall training speed: 298 iterations in 0:00:56 (0.1903 s / it)\n",
            "\u001b[32m[11/09 13:07:16 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:58 (0:00:01 on hooks)\n",
            "\u001b[32m[11/09 13:07:16 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./output/model_final.pth ...\n"
          ]
        }
      ],
      "source": [
        "# Initialize a dataset from the release file\n",
        "model = pipe.train_segmentation_model(TRAIN_DATASET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8MsoYDLduR8"
      },
      "source": [
        "#### Create a new dataset and upload images to him"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK8SfINidxzP",
        "outputId": "3438e95d-4c34-4fb5-e573-5069fd59d4bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='cannabis_patches_week9_15_06_2023_3x_regular_IMG_2129' full_name='etaylor/cannabis_patches_week9_15_06_2023_3x_regular_IMG_2129' cloned_from=None description='cannabis patches week=week9_15_06_2023 zoom_type=3x_regular of image=IMG_2129.' category='other' public=False owner=Owner(username='etaylor', created_at='2022-12-28T12:53:18Z', email=None) created_at='2023-11-08T15:15:37.216018Z' enable_ratings=False enable_skip_labeling=True enable_skip_reviewing=False enable_save_button=False enable_label_status_verified=False enable_same_dimensions_track_constraint=False enable_interpolation=True task_type='segmentation-bitmap' label_stats=LabelStats(TOTAL=None, LABELED=None, UNLABELED=None, PRELABELED=None) samples_count=0 collaborators_count=None task_attributes=TaskAttributes(format_version='0.1', categories=[TaskAttributeCategory(name='trichome', id=1, color=None, has_instances=None, attributes=None, dimensions=None)], image_attributes=None) labelsets=None role=None readme='' metadata={} noncollaborator_can_label=False noncollaborator_can_review=False embeddings_enabled=None\n",
            "Uploaded IMG_2129_p0.png and added as sample: uuid='43d91c77-5e09-48db-969b-a92b5baad377' name='IMG_2129_p0.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/7fb15ce2-8307-4b6b-a663-c1890ac89377.png')) metadata={} created_at='2023-11-08T15:15:38.520631Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p1.png and added as sample: uuid='36c6f260-c26c-425a-905e-16da6471aa38' name='IMG_2129_p1.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/f9a7c631-b99e-40ce-acfc-8eefa9c8deee.png')) metadata={} created_at='2023-11-08T15:15:38.989569Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p2.png and added as sample: uuid='08b16008-6845-4ede-8512-efb507c09dbf' name='IMG_2129_p2.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/a0befd56-d87b-4e76-a69c-878201a920fb.png')) metadata={} created_at='2023-11-08T15:15:39.464329Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p3.png and added as sample: uuid='19559c20-cf78-46c1-b742-152336a07c93' name='IMG_2129_p3.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/d8231358-5615-4f62-9e08-cc8b13bf7c4a.png')) metadata={} created_at='2023-11-08T15:15:39.955133Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p4.png and added as sample: uuid='552f0316-765a-4511-8857-706e32ee55f0' name='IMG_2129_p4.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/c3cbf771-e137-419b-8d4e-67481be34fa1.png')) metadata={} created_at='2023-11-08T15:15:40.450055Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p5.png and added as sample: uuid='ac83e910-f026-4861-ba8f-8e9c874d38c4' name='IMG_2129_p5.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/1dbff1ac-20a6-4562-aa0a-4aa1c842e4df.png')) metadata={} created_at='2023-11-08T15:15:40.825332Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p6.png and added as sample: uuid='9d9baa17-5ea8-47b5-961a-430ad58c22cb' name='IMG_2129_p6.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/91adb9aa-6cbc-4c20-bc78-e78d007e158d.png')) metadata={} created_at='2023-11-08T15:15:41.332728Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p7.png and added as sample: uuid='7dc41dea-c848-4828-852e-9fef41398702' name='IMG_2129_p7.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/9509ee43-0b43-4736-ab7e-6c45e715a336.png')) metadata={} created_at='2023-11-08T15:15:41.821197Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p8.png and added as sample: uuid='3d2eb8e8-e9a0-47a8-a682-6607191972ae' name='IMG_2129_p8.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/6114cdcf-87f9-4acc-b080-d4485948b4c7.png')) metadata={} created_at='2023-11-08T15:15:42.274118Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p9.png and added as sample: uuid='008af35c-a9ec-42bc-9caf-3ab8017c32d6' name='IMG_2129_p9.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/0f35992b-4747-4cfb-8c5e-e911c0f317e5.png')) metadata={} created_at='2023-11-08T15:15:42.658264Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p10.png and added as sample: uuid='d90c9f3d-953b-49dc-9c9b-778e66927d60' name='IMG_2129_p10.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/feac9fd4-8f44-4cad-bbbf-b0504006c547.png')) metadata={} created_at='2023-11-08T15:15:43.103215Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p11.png and added as sample: uuid='1428e93a-4b3d-4904-b765-bb4bcfc6f84e' name='IMG_2129_p11.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/c16666bb-dfde-4df7-bf41-056a10518c35.png')) metadata={} created_at='2023-11-08T15:15:43.570494Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p12.png and added as sample: uuid='be4ac00a-1265-4c99-9e11-4550774d2df1' name='IMG_2129_p12.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/4522b28c-ac86-403a-8628-730948950680.png')) metadata={} created_at='2023-11-08T15:15:44.048613Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p13.png and added as sample: uuid='3be950aa-11b2-4775-8789-8fd919de6909' name='IMG_2129_p13.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/c1c38abb-2980-46f5-a0b6-1c5d4ffa4a3e.png')) metadata={} created_at='2023-11-08T15:15:44.536382Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p14.png and added as sample: uuid='0290be89-075b-4995-9a29-ff79207f2979' name='IMG_2129_p14.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/68ff44c8-ca61-4e43-b431-a0ada70ad7d6.png')) metadata={} created_at='2023-11-08T15:15:45.016796Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n",
            "Uploaded IMG_2129_p15.png and added as sample: uuid='86a4defa-7cfd-4d99-a902-72d3e888c9c0' name='IMG_2129_p15.png' attributes=ImageSampleAttributes(image=URL(url='https://segmentsai-prod.s3.eu-west-2.amazonaws.com/assets/etaylor/1a56b9f9-af87-4bee-bea8-f657ca1fa5f5.png')) metadata={} created_at='2023-11-08T15:15:45.517707Z' created_by='etaylor' assigned_labeler=None assigned_reviewer=None comments=[] priority=0.0 has_embedding=False label=None issues=None dataset_full_name=None\n"
          ]
        }
      ],
      "source": [
        "# Create a new test dataset for the specified image, week, and zoom type\n",
        "test_dataset = pipe.create_new_test_dataset(image_name, config.WEEKS_DIR[week], config.ZOOM_TYPES_DIR[zoom_type])\n",
        "# Get the absolute path to the processed image\n",
        "abs_images_path = f\"{config.get_processed_cannabis_image_path(week, zoom_type)}/{image_name}\"\n",
        "\n",
        "# Upload the images that are not annotated to the dataset\n",
        "segmentsai_handler.upload_images(test_dataset, abs_images_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "release_name = \"v0.1\"\n",
        "description = \"upload predictions to dataset.\"\n",
        "segmentsai_handler.client.add_release(test_dataset, release_name, description)\n",
        "test_release = segmentsai_handler.client.get_release(test_dataset, \"v0.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpX-71UwlW6o"
      },
      "source": [
        "### Upload the Images that are not annotaed to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gozhWa6Ahvxm",
        "outputId": "f73115aa-9519-491c-ac9d-8da8a6b8b70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "release=uuid='9620b94f-75a0-446a-89a6-f65728abdf92' name='v0.1' description='upload predictions to dataset.' release_type='JSON' attributes=URL(url='https://segmentsai-prod.s3.amazonaws.com/releases/9620b94f-75a0-446a-89a6-f65728abdf92.json?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5RYRXRX22Y5Q2VMR%2F20231108%2Feu-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231108T151832Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=1bb69b40766451c99e2b7d70faa9ebbd2002db475cbe575329d6cc657db1e0a5') status='SUCCEEDED' created_at='2023-11-08T15:17:14.050898Z' samples_count=16\n",
            "Initializing dataset...\n",
            "Preloading all samples. This may take a while...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[38;2;255;153;0m██████████\u001b[0m| 16/16 [00:02<00:00,  6.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized dataset with 16 images.\n"
          ]
        }
      ],
      "source": [
        "pipe.upload_predictions(test_release, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the dataset - ONLY IF YOU WANT TO DELETE THE DATASET!!!\n",
        "DELETE_DATASET_NAME = \"\"\n",
        "segmentsai_handler.client.delete_dataset(DELETE_DATASET_NAME)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
